{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50d25dea-bcce-4c9a-b95f-32a2ae58606d",
   "metadata": {},
   "source": [
    "# ETL Pipelines 4 Data Professionals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1418debb-7181-419f-9f85-f5aab834e1da",
   "metadata": {},
   "source": [
    "> ‚ÄúSin una forma sistem√°tica de iniciar y mantener los datos limpios, los datos incorrectos ocurrir√°n.‚Äù ‚Äî Donato Diorio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5154739-1575-4853-9189-e49f044cd76f",
   "metadata": {},
   "source": [
    "![data_flow](https://cdn.dribbble.com/users/1752792/screenshots/5652276/media/12db9ebc672c30dcb4d0fd125f70fb41.png)\n",
    "\n",
    "Source: [Mindaugas Sad≈´nas](https://dribbble.com/shots/5652276-User-flow/attachments/10982649?mode=media)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f1e3e1-e179-47ab-bf0d-5c3cd0ee093f",
   "metadata": {},
   "source": [
    "## Resultados del Aprendizaje\n",
    "\n",
    "Al final de este taller,\n",
    "1. Entenderas mejor por que necesitamos mover los datos de un punto a otro a la misma ves que los limpiamos.\n",
    "2. Comprenderas como combinar datos que vienen de diferentes fuentes\n",
    "3. Tendras el conocimiento de como crear tus proprias tuberias de datos con Python\n",
    "4. Aprenderas un poco mas como manipular y moldear tus datos en la forma en que los necesitas.\n",
    "5. Entenderas como visualisar las tuberias que creas para ayudarte en su desarrollo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4c48f3-0654-440b-b630-1c19519f0ce2",
   "metadata": {},
   "source": [
    "## Tabla de Contenidos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e2be28-d7fe-49b0-801f-b5a269a91016",
   "metadata": {},
   "source": [
    "1. Que son las Tuberias ETL y Por Que Deberias Aprender a Crearlas?\n",
    "2. Herramientas Para la Sesion\n",
    "3. Nuestro Caso Para Este Taller\n",
    "4. Datos\n",
    "5. Tuberias Peque√±as con üêº's `pipe`\n",
    "6. Extraer\n",
    "7. Transformar\n",
    "8. Descargar\n",
    "9. Lanza La Tuberia\n",
    "10. Automatizar\n",
    "11. Resumen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629dc5e9-e52e-43a9-8843-d93e7e0b9e9f",
   "metadata": {},
   "source": [
    "## 1. Que son las Tuberias ETL y Por Que Deberias Aprender a Crearlas?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427c3972-16d8-4db1-850d-8d0ea08cc875",
   "metadata": {},
   "source": [
    "![etl_pipe](https://databricks.com/wp-content/uploads/2021/05/ETL-Process.jpg)\n",
    "\n",
    "**Que son las Tuberias ETL?**\n",
    "\n",
    "El acr√≥nimo ETL significa Extract (Extraer), Transform (Transformar), y Load (Descargar). Este es el proceso por el cual pasan los datos que consumimos como analistas, cient√≠ficos de datos, investigadores cient√≠ficos, ect..., antes de que lleguen a nuestras manos.\n",
    "\n",
    "**Por Que Deberias Aprender a Crearlas?**\n",
    "\n",
    "Como profesionales de datos, nuestro tarea es crear valor para nuestras organizaciones, nuestros clientes y nuestros colaboradores usando todos los datos que tengamos a nuestra disposicion. Sin embargo, para sacarle el maximo provecho a los datos que tenemos a mano, necesitamos\n",
    "1. Informacion sobre proceso por el cual se generaron los datos, Por ejemplo,\n",
    "    - Punto de ventas\n",
    "    - Clicks en un mercado en l√≠nea como Amazon, Etzy, Ebay, ect.\n",
    "    - Estudio epidemiol√≥gico\n",
    "    - ...\n",
    "2. Informacion acerca de las transformaciones que ocurrieron durante el proceso de limpieza y combinacion. Por ejemplo,\n",
    "    - Grados Celcius fueron convertidos a fahrenheit\n",
    "    - Precios en pesos Chilenos fueron convertidos a {inserta tu üí∏ preferida}\n",
    "    - Observaciones no numericas y no disponibles ahora contienen \"No Disponible\"\n",
    "    - Observaciones numericas ahora contienen el valor promedio de su respectiva variable, por ejemplo, un variable con el salario de todos los empleados de una compa√±√≠a ahora contiene $40,000$/a√±o USD en los valores que no estaban disponibles\n",
    "    - ...\n",
    "3. Informacion acerca de como se almacenaron los datos y en donde. Por ejemplo,\n",
    "    - Parquet format\n",
    "    - NOSQL or SQL base de datos\n",
    "    - CSV\n",
    "    - ...\n",
    "\n",
    "El entender como fluyen los tres procesos descritos arriba nos ayudara a tener mas conocimiento acerca de los datos que vamos a usar, y una de las mejores maneras para entender ese proceso es a traves de la creacion de tuberias de datos.\n",
    "\n",
    "**Cuales Profesionales de Datos Usan Estas Tuberias?**\n",
    "\n",
    "- Cientificos de datos\n",
    "- Analistas de datos\n",
    "- Ingenieros de datos\n",
    "- Machine Learning Engineers\n",
    "- Programadores\n",
    "- DevOps Engineers\n",
    "- Investigadores de Ciencias Sociales\n",
    "\n",
    "En conclusion, entender como fluyen los datos en tu organizacion te ayudara a\n",
    "- Utilizar los datos limpios para tus analysis mientras dejas los datos originales intactos.\n",
    "- Detectar inconsistencias en los datos originales.\n",
    "- Usar el tiempo que tienes para analizar e informar sobre tus hallazgos de manera mas eficiente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4fa45a-d839-4b7a-a9b9-56c4c7823071",
   "metadata": {},
   "source": [
    "## 2. Herramientas Para la Sesion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebe98bf-cccc-4817-9c95-fe4a9e874049",
   "metadata": {},
   "source": [
    "Las herramientas que utilizaremos en el taller son las siguientes.\n",
    "\n",
    "- [pandas](https://pandas.pydata.org/) - \"es una herramienta de an√°lisis y manipulaci√≥n de datos de c√≥digo abierto r√°pida, potente, flexible y f√°cil de usar, construida sobre el lenguaje de programaci√≥n Python.\"\n",
    "- [Prefect](https://docs.prefect.io/) - \"es un nuevo sistema de gesti√≥n de flujo de trabajos, dise√±ado para una infraestructura moderna e impulsado por el motor de flujo de trabajo de c√≥digo abierto llamado, Prefect Core. Los usuarios organizan las tareas en `Tasks` y `Flows`, y Prefect se encarga del resto.\"\n",
    "- [sqlite3](https://docs.python.org/3/library/sqlite3.html) - \"SQLite es una biblioteca escrita en C que proporciona una base de datos ligera basada en disco que no requiere un proceso de servidor independiente y permite acceder a la base de datos mediante una variante no est√°ndar del lenguaje de consulta SQL.\"\n",
    "\n",
    "Antes de continuar, carguemos los modulos que necesitaremos y examinemos un ejemplo de prefect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8218bb9-af89-42e8-a8b2-da3f72d11b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from prefect import task, Flow\n",
    "import sqlite3\n",
    "from os.path import join\n",
    "from contextlib import closing\n",
    "from prefect.tasks.database.sqlite import SQLiteScript\n",
    "\n",
    "pd.options.display.max_rows = None\n",
    "pd.options.display.max_columns = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b1fe20-0851-4625-a154-a57ca7e2f5d4",
   "metadata": {},
   "source": [
    "Imaginate que tenemos unos datos acerca de todos incendios forestales entre 1983-2020 en los Estados Unidos.\n",
    "\n",
    "Puedes encontrar mas informacion sober los datos [aqui](https://www.kaggle.com/kkhandekar/total-wildfires-acres-affected-1983-2020)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1565fb-8f3f-438b-ae13-3eccd36dd13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(join(\"..\", \"data\", \"example\",\n",
    "                 \"Federal Firefighting Costs (Suppression Only).csv\")).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c8d481-fc31-4b3d-a4f2-9e50ff7ae90c",
   "metadata": {},
   "source": [
    "Como puedes ver, la majoria de las variables necesitan un poco de arreglo ya que en Python no podemos, por ejemplo, numeros con formatos como `$70,890`. Tambien, ya que necesitaremos los nuevos datos todos los meses, crearemos una tuberia ETL para no tener que repetir el proceso de nuevo.\n",
    "\n",
    "Cuando usas prefect tienes dos API's importantes, una es `task` y la otra es `Flow`. `task` se usa como un decorador arriba de funciones y te permite decirle a prefect que esa funcion tomara parte en tu tuberia de datos a traves del `Flow` API.\n",
    "\n",
    "Por ejemplo, creemos 3 funciones, una que extraiga los datos que necesitamos, otra que los limpie, y otra que los descargue, y pongamoles a las 3 el decorados `task`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2191cb3-b271-4382-8fbe-9da3bbae23f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = join(\"..\", \"data\", \"example\", \"Federal Firefighting Costs (Suppression Only).csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf3ccc3-a96d-4b84-aa6c-6325ac731153",
   "metadata": {},
   "outputs": [],
   "source": [
    "@task\n",
    "def extract(path):\n",
    "    return pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43947f55-bdce-4308-93eb-dcdf533ae144",
   "metadata": {},
   "source": [
    "Como viste arriba, solo las ultimas 5 variables tienen comas (`,`) y simbolos de dinero (`$`) asi que crearemos un `for` loop y a cada una de las variables les reemplazaremos ambas por un espacio vacio (`\"\"`).\n",
    "\n",
    "Para el proceso de descarga, guardaremos los datos en el formato `parquet`. Este es uno de los formatos mas populares ya que tiene una orientacion columnar en ves de por fila.\n",
    "\n",
    "![colvsrow](https://3.bp.blogspot.com/-3aUydn8zCsQ/VjslzWCu3pI/AAAAAAAAAI8/XOi77xQNmm0/s1600/Difference-between-Column-based-and-Row-based-Tables.png)\n",
    "\n",
    "Source: [SAP HANA Central](http://www.hanaexam.com/p/row-store-vs-column-store.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b387cd-90c7-47a4-9d20-3655f9032514",
   "metadata": {},
   "outputs": [],
   "source": [
    "@task\n",
    "def transform(data):\n",
    "    for col in data.iloc[:, 1:].columns:\n",
    "        data[col] = data[col].str.replace(',', '').str.replace('$', '').astype(int)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5011755-2449-4c78-b9fc-45d4ef8e532b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@task\n",
    "def load(data, path):\n",
    "    data.to_parquet(path, compression='snappy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8242354e-4d66-4746-b3cb-cf059e0a8eaa",
   "metadata": {},
   "source": [
    "Cuando tenemos todos los pasos listos, creamos un gestor de contexto en Python usando el `Flow` API. A esta funcion le podemos dar un nombre, por ejemplo, `\"Ejemplo ETL\"` y despues asignar lo que pasa adentro del contexto a una variable de nombre `flow` (sin mayuscula). Adentro del contexto podemos instanciar nuestras 3 funciones y enlazar una con la otra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7b1ec0-0e40-4d5d-94a8-b00118452024",
   "metadata": {},
   "outputs": [],
   "source": [
    "with Flow(\"Ejemplo ETL\") as flow:\n",
    "    data = extract()\n",
    "    data_clean = transform(data)\n",
    "    load(data_clean, join(\"..\", \"data\", \"example\", \"my_test.parquet\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350106b9-921a-46c4-8c06-5eade6ab4a37",
   "metadata": {},
   "source": [
    "Puedes ver el resultado de los pasos a seguir en nuestra tuberia usando `flow.visualize()` y puedes iniciarla con `flow.run()`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18cb5b3c-8417-4d13-8d15-b2a649282870",
   "metadata": {},
   "outputs": [],
   "source": [
    "flow.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda05bfa-08ab-4713-b801-8c61c44545ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "flow.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826f9819-bbab-4e18-9a74-4ee142ce048f",
   "metadata": {},
   "source": [
    "Para cerciorarnos de que tenemos los datos correctos, creemos una visualisacion con pandas y hvplot que nos permite agregar interactividad a nuestros graficos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a32d483-96c3-4998-ad8f-6dd6fd4d3297",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hvplot.pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7bd5051-0822-4448-8e8e-d82fa2221b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_parquet(join(\"..\", \"data\", \"example\", \"my_test.parquet\")).hvplot(x='Year', y=\"ForestService\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734270ca-0595-4989-8e39-707d5a6aee2c",
   "metadata": {},
   "source": [
    "## 3. Nuestro Caso Para Este Taller"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33c315e-7854-4149-b7d4-11be818d533a",
   "metadata": {},
   "source": [
    "Imagina que trabajas para una consultoria de ciencias de datos que se llama, Beautiful Analytics. Tu jefa te dice que tiene un projecto para ti en el cual trabajaras para tres gobiernos usando datos sobre las bicicletas compartidas en las ciudades de Londres (England, UK), Seoul (South Korea), y Washington (DC, USA). El problema que cada gobierno quere resolver es el mismo,\n",
    "\n",
    "**Desafio # 1**\n",
    "\n",
    "> cuantas bicicletas necesitamos mantener disponible en la ciudad a cada hora durante los proximos a√±os?\n",
    "\n",
    "Cada gobierno captura datos similares pero, como ya te puedes imaginar, todos usan palabras y medidas diferentes en referencia a la misma variable. Lo que quiere decir que nuestro primer trabajo antes de poder responder la prgunta de arriba es, arreglar los datos y ponerlos de una manera en la que los podamos usar mejor. De paso, lo que de verdad nos ayudaria un monton es automatizar la extraccion, transformacion y descarga de nuestros datos cuando ya esten limpios, ya que en el futuro seguiremos recibiendo los datos por parte de los gobiernos. Esto quiere decir que nuestro primer real problema es,\n",
    "\n",
    "**Desafio # 0**\n",
    "\n",
    "> Crea una tuberia de datos que extraiga, transforme y descargue los datos necesarios."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e6fc60-7066-4e65-8d10-1f020aeea86f",
   "metadata": {},
   "source": [
    "## 4. Datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487b3a1f-5b47-47f9-af36-17032159df12",
   "metadata": {},
   "source": [
    "![bikes](https://camo.githubusercontent.com/87d0f6a329d5dd8915136dcf9b121b789bfa613abac31d591f5629cdfb072595/68747470733a2f2f696d672e6b6f72656174696d65732e636f2e6b722f75706c6f61642f6e65777356322f696d616765732f3230323130332f33653962353830316334333034386563613331623333303931373663386461392e6a7067)\n",
    "\n",
    "Los tres archivos de datos contienen informacion similar acerca de cuantas bicicletas se han necesitado a cada hora durante varios anos, para cada ciudad.\n",
    "\n",
    "Puedes obtener mas informacion acerca de los datos de cada ciudad usando los siguientes enlaces.\n",
    "\n",
    "- [Seoul, Korea del Sur](https://archive.ics.uci.edu/ml/datasets/Seoul+Bike+Sharing+Demand#)\n",
    "- [London, England, UK](https://www.kaggle.com/hmavrodiev/london-bike-sharing-dataset)\n",
    "- [Washington, DC, USA](https://www.kaggle.com/marklvl/bike-sharing-dataset?select=hour.csv)\n",
    "\n",
    "Aqui estan las variables que aparecen and los tres archivos de datos.\n",
    "\n",
    "| London | Seoul | Washington |\n",
    "|:------:|:------:|:------:|\n",
    "| date            | date            | instant   |\n",
    "| count           | count           | date      |\n",
    "| temperature     | hour            | seasons   |\n",
    "| temp_feels_like | temperature     | year      |\n",
    "| humidity        | humidity        | month     |\n",
    "| wind_speed      | wind_speed      | hour           |\n",
    "| weather_code    | visibility      | is_holiday     |\n",
    "| is_holiday      | dew_point_temp  | weekday        |\n",
    "| is_weekend      | solar_radiation | workingday     |\n",
    "| seasons         | rainfall        | weathersit     |\n",
    "|                 | snowfall        | temperature    |\n",
    "|                 | seasons         | temp_feels_like |\n",
    "|                 | is_holiday      | humidity        |\n",
    "|                 | functioning_day | wind_speed      |\n",
    "|                 |                 | casual     |\n",
    "|                 |                 | registered |\n",
    "|                 |                 | count      |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c09bdcb-8f1d-4841-a257-47189dca98c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "london_path = join('..', 'data', 'raw', 'london', 'london_bikes.db')\n",
    "seoul_path = join('..', 'data', 'raw', 'seoul', 'SeoulBikeData.csv')\n",
    "wash_dc_path = join('..', 'data', 'raw', 'wash_dc', 'washington.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de08ae5-aa71-4595-9ef4-ae2dc46f6e5f",
   "metadata": {},
   "source": [
    "Necesitaremos guardar nuestro nuevo archivo en una base de datos o en un formato en el cual se nos facilite tanto lo que ocupa el archivo como la velocidad con la que lo podemos abrir e utilizar. Por eso crearemos dos caminos y dos nombres para los archivos que usaremos luego."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc80b44-ad24-4a93-9b30-093bbbab7969",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_path = join('..', 'data', 'processed', 'clean.parquet')\n",
    "clean_db_path = join('..', 'data', 'processed', 'bikes.db')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3395aa14-9426-487a-91e4-cfb233ba8c3b",
   "metadata": {},
   "source": [
    "Los datos que tenemos de las bicicletas en Londres estan en una base de datos SQLite y para leerlos primero necesitamos crear una coneccion a la base de datos. El siguiente paso es usar la funcion de pandas `read_sql_query` para leer los datos. Esta funcion toma como argumento dos cosas, el programa para agarrar los datos y la coneccion a la base de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef427f4a-d37e-4944-b144-465a5eb440aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect(london_path)\n",
    "query = \"SELECT * FROM uk_bikes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af05647-6c40-4ae2-8f11-3b086ddcc4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "london = pd.read_sql_query(query, conn)\n",
    "london.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed81210-3ce1-418f-9250-ff748b400be4",
   "metadata": {},
   "source": [
    "Los datos de Seoul estan en forma de texto y separados por comas, y los datos de Washington estan en el formato JSON. Para estos dos podemos utilizar `pd.read_csv` y `pd.read_json`, respectivamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67208993-6c95-42eb-afa3-fe35c7aebeda",
   "metadata": {},
   "outputs": [],
   "source": [
    "seoul = pd.read_csv(seoul_path)\n",
    "seoul.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e583e5-3b38-4f8f-b7ef-835cc93c5dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "washington = pd.read_json(wash_dc_path)\n",
    "washington.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4253750-4e7e-40c5-a2df-193580060fe7",
   "metadata": {},
   "source": [
    "## 5. Tuberias de datos con üêº's `pipe`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122909c8-0e1d-4650-8a45-f2ef73495caa",
   "metadata": {},
   "source": [
    "![](https://camo.githubusercontent.com/45ae53e215244585378c3e414ce05abb4f5f6be3/68747470733a2f2f6d656469612e67697068792e636f6d2f6d656469612f4978365150753533576c4236772f67697068792e676966)\n",
    "\n",
    "El operador `pipe` es una funci√≥n de pandas que te permite encadenar operaciones que toman un conjunto de datos, lo modifican y te devuelven la versi√≥n modificada de los datos originales. En esencia, nos permite mover los datos a trav√©s de una serie de pasos hasta que alcancemos la estructura que deseamos.\n",
    "\n",
    "Por ejemplo, imaginate que tenemos un grupo de datos y 4 funciones para arreglarlo, la cadena se veria de la siguiente manera.\n",
    "\n",
    "```python \n",
    "(data.pipe(change_cols, list_of_cols)\n",
    "     .pipe(clean_numeric_vars, list_of_numeric_vars)\n",
    "     .pipe(add_dates_and_location, 'Auckland', 'NZ')\n",
    "     .pipe(fix_and_drop, 'column_to_fix', seasons_NZ, cols_drop_NZ))\n",
    "```\n",
    "\n",
    "Otra manera de visualizar lo que sucede con pandas' `pipe` es a traves de la siguiente imagen.\n",
    "\n",
    "![img](images/pandas_pipe.png)\n",
    "\n",
    "Empecemos con un peque√±o ejemplo primero sin usar `pipe`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6084c16c-8acc-4cc7-8d1c-b476bf8e2713",
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_data = pd.DataFrame({\"Postal Codes\": [22345, 32442, 20007], \n",
    "                         \"Cities\": [\"Miami\", \"Dallas\", \"Washington\"],\n",
    "                         \"Date\": pd.date_range(start='9/27/2021', periods=3)})\n",
    "toy_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63c6722-7611-416a-b77b-aa8653f38e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_cols(data, cols_list):\n",
    "    data.columns = cols_list\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834ed627-12be-4d5d-9fd1-ca30474bc8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "change_cols(toy_data, [\"postal_code\", \"city\", \"date\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181234f5-44bf-47b3-a754-907f8609c644",
   "metadata": {},
   "source": [
    "Como puedes ver, con una solo funcion no tiene mucho sentido pasarla por el `pipe` pero con una cadena de funciones, la historia cambia.\n",
    "\n",
    "Y a que tenemos columnas con nombres differentes, creemos 3 listas con los mismos nombres para los tres archivos de datos ya que pronto los necesitaremos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967e5dac-7964-40f9-a711-29fb92f22508",
   "metadata": {},
   "outputs": [],
   "source": [
    "london_cols = ['date', 'count', 'temperature', 'temp_feels_like', 'humidity', 'wind_speed', 'weather_code', 'is_holiday', 'is_weekend', 'seasons']\n",
    "seoul_cols = ['date', 'count', 'hour', 'temperature', 'humidity', 'wind_speed', 'visibility', 'dew_point_temp', 'solar_radiation', 'rainfall', 'snowfall', 'seasons', 'is_holiday', 'functioning_day']\n",
    "wa_dc_cols = ['instant', 'date', 'seasons', 'year', 'month', 'hour', 'is_holiday', 'weekday', 'workingday', 'weathersit', 'temperature', 'temp_feels_like', 'humidity', 'wind_speed', 'casual', 'registered', 'count']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2fedbc-fb57-435d-8e68-e637a2366475",
   "metadata": {},
   "source": [
    "Lo siguiente que queremos hacer es agregar informacion adicional acerca de las fechas que tenemos para cada archivo. Esto lo podemos lograr despues de convertir la variable `date` a formato `datetime`, el cual nos permitira accesar el a√±o, mes, semana, ect. de adentro de cada fecha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71d8b3c-50bc-4659-8a50-2f7dc92eef97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_dates_and_location(data, city, country):\n",
    "    \n",
    "    data['date'] = pd.to_datetime(data['date'])\n",
    "    data[\"year\"] = data['date'].dt.year\n",
    "    data[\"month\"] = data['date'].dt.month\n",
    "    data[\"week\"] = data['date'].dt.isocalendar().week.astype(int)\n",
    "    data[\"day\"] = data['date'].dt.day\n",
    "    data[\"hour\"] = data['date'].dt.hour\n",
    "    data[\"weekday\"] = data['date'].dt.dayofweek\n",
    "    data[\"is_weekend\"] = (data[\"weekday\"] > 4).astype(int)\n",
    "    data['date'] = data['date'].dt.date\n",
    "    data['city'] = city\n",
    "    data['country'] = country\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3968e8ef-b70a-44a6-83c0-dae966e96570",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_dates_and_location(toy_data, \"Sydney\", \"AU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9545f2-af3d-4933-b031-e6c00a2f2e67",
   "metadata": {},
   "source": [
    "Como puedes ver, agregamos un monton de informacion a nuestros datos con una simple funcion, pero que pasa cuando queremos encadenar dos o tres or cuatro? Lo siguiente no estaria muy facil de leer cierto? `add_dates_and_location(change_cols(toy_data, [\"postal_code\", \"city\", \"date\"]), \"Sydney\", \"AU\")`. Movamonos al `pipe` operator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d83cb9-dfd4-432e-9461-534be6d0cbd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_data = pd.DataFrame({\"Postal Codes\": [22345, 32442, 20007], \n",
    "                         \"Cities\": [\"Miami\", \"Dallas\", \"Washington\"],\n",
    "                         \"Date\": pd.date_range(start='9/27/2021', periods=3)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd338191-61ef-4db6-a537-20e8fa86a3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    toy_data.pipe(change_cols, [\"zip_code\", \"city\", \"date\"])\n",
    "            .pipe(add_dates_and_location, \"Sydney\", \"AU\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d4c21b-2919-4d16-94e0-8b3fea8697c4",
   "metadata": {},
   "source": [
    "Como puedes ver, ahora la cadena de nuestras funciones es mas legible que antes y podemos continuar y encadenar aun mas funciones en este mismo estylo.\n",
    "\n",
    "En nuestros datos tenemos las etapas del a√±o con diferentes nombres y tambien tenemos variables que no necesitamos o que no estan en los tres archivos. Arreglemos ambas!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2128c78-d157-4e81-96a4-4cf976290e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "seasons_london = {0: 'Spring', 1: 'Summer', 2: 'Fall', 3: 'Winter'}\n",
    "seasons_wa_dc = {1: 'Spring', 2: 'Summer', 3: 'Fall', 4: 'Winter'}\n",
    "holidays_seoul = {'No Holiday': 0, 'Holiday': 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8816693-17ef-4338-910f-53f607ea0f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_drop_london = ['temp_feels_like', 'weather_code']\n",
    "cols_drop_seoul = ['visibility', 'dew_point_temp', 'solar_radiation', 'rainfall', 'snowfall', 'functioning_day']\n",
    "cols_drop_wa_dc = ['instant', 'temp_feels_like', 'casual', 'registered', 'workingday', 'weathersit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07cb7869-8908-4c59-a8de-0ba79f7103c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_and_drop(data, col_to_fix, mapping, cols_to_drop):\n",
    "    data[col_to_fix] = data[col_to_fix].map(mapping)\n",
    "    return data.drop(cols_to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c47580-aed9-4fba-9e14-c5c2210bcdcd",
   "metadata": {},
   "source": [
    "Probemos el `pipe` pero con los datos de Washington, DC ahora y con la lista de columnas que creamos hace rato."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f210a7ef-0e37-4c65-aec1-8d6986c2239b",
   "metadata": {},
   "outputs": [],
   "source": [
    "washington.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e61ae55-01ad-4fda-8d7e-d593bd00141e",
   "metadata": {},
   "outputs": [],
   "source": [
    "(washington.pipe(change_cols, wa_dc_cols)\n",
    "           .pipe(fix_and_drop, 'seasons', seasons_wa_dc, cols_drop_wa_dc)).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4022be4a-27dd-450b-9a04-3e79998b6c11",
   "metadata": {},
   "source": [
    "Por √∫ltimo, necesitamos normalizar los datos para Washington DC ya que las columnas se han alterado un poco."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddefe01c-40fc-4338-80cd-200488d0dbc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_vars(data):\n",
    "    data['temperature'] = data['temperature'].apply(lambda x: (x * 47) - 8)\n",
    "    data['humidity'] = data['humidity'].apply(lambda x: (x / 100))\n",
    "    data['wind_speed'] = data['wind_speed'].apply(lambda x: (x / 67))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3ce8a2-4e9c-4569-ac60-6b0b7e1397cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(path):\n",
    "    return pd.read_json(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ea87cb-97a8-414d-b72f-9e9d3192ee3a",
   "metadata": {},
   "source": [
    "Finalmente, podemos usar nuestro operador de tuber√≠a de nuevo para completar el proceso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90f03e1-392b-48cc-b2e3-9174eaac265c",
   "metadata": {},
   "outputs": [],
   "source": [
    "washington = (extract_data(wash_dc_path).pipe(change_cols, wa_dc_cols)\n",
    "                                        .pipe(add_dates_and_location, 'DC', 'USA')\n",
    "                                        .pipe(fix_and_drop, 'seasons', seasons_wa_dc, cols_drop_wa_dc)\n",
    "                                        .pipe(normalize_vars))\n",
    "washington.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6b2855-b75f-418c-b9a3-9bfc3d8670d3",
   "metadata": {},
   "source": [
    "## Ejercicio\n",
    "\n",
    "1. Crea una funcion para extraer los datos de Londres.\n",
    "2. Crea una tuberia de datos similar a la de Washington usando pandas' `pipe`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86463bad-efc6-4ac7-bb2a-e5991436ca29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d60161-f8f1-4cff-be77-ecd7c0280ebe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28377eea-aa32-4775-b1fd-6adeda091065",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733222da-f446-4ac5-9f24-5c07b3b345a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5d4e2dec-e422-4514-b1d2-184dcdd845ef",
   "metadata": {},
   "source": [
    "## 6. Extraer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf7443d-f824-4be4-bf6d-f9b3f64fae2b",
   "metadata": {},
   "source": [
    "Dependiendo de donde esten los datos, en que formato esten almacenados, y como podemos accesarlos, este puede ser uno de los pasos mas cortos tanto como el mas largo en nuestra tuberia de datos. Aqui estan algunos de los formatos que podrias encontrar en tu dia a dia.\n",
    "\n",
    "- Texto: usualmente el formato de texto es similar al que vemos en Microsoft Excel pero sin formulas o graficos. Por ejemplo, CVS o TSV.\n",
    "- JSON: JavaScript Object Notation es un sub-lenguage bastante popular por su sintactica simple\n",
    "- Databases: Estas pueden ser SQL, NOSQL, MPP (massively parallel processing), entre otras.\n",
    "- GeoJSON: Es un tipo de formato para datos que contienen informacion geografica. Existen muchos mas tipos de datos para GIS.\n",
    "- HTML: Se refiere a Hyper Text Markup Language y representa el esqueleto de casi todas las paginas web en existencia.\n",
    "- ...\n",
    "\n",
    "Ya que aprendimos a como crear tuberias con pandas, ahora necesitamos crear funciones para nuestra tuberia ETL principal y esto lo podemos lograr usando el decorador de prefect, `@task`. Este decorador recuerda las funciones que queremos enlazar y nos ayuda a crear una red en la cual cada nodo es una funcion y cada enlaze conecta una o mas funciones una sola vez.\n",
    "\n",
    "Recuerda, al decorador `@task` es una funcion dentro de prefect y como tal, le podemos pasar varios argumentos que nos ayudan a modificar el comportamiento de cada function en nuestra tuberia.\n",
    "\n",
    "Puedes aprender mas acerca del `task` API en los [documentos oficiales aqui](https://docs.prefect.io/core/concepts/tasks.html#overview)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d35f3c5-1e1e-43f2-9ba5-5c320032b4f6",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "task??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f935dd83-0de2-49ab-aaac-ec7bcf1586ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "@task\n",
    "def extract_1(path):\n",
    "    return pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b63f485-5ec4-49ab-98c7-133bd50e840b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@task\n",
    "def extract_2(path):\n",
    "    conn = sqlite3.connect(path)\n",
    "    query = \"SELECT * FROM uk_bikes\"\n",
    "    return pd.read_sql_query(query, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4b97de-bacd-4700-947f-a283107ae666",
   "metadata": {},
   "outputs": [],
   "source": [
    "@task\n",
    "def extract_3(path):\n",
    "    return pd.read_json(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca428a5-d2de-4779-814b-442656e8298e",
   "metadata": {},
   "source": [
    "## 7. Transformar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c64fc00-a93c-4270-a9a2-b75e96eb5cfa",
   "metadata": {},
   "source": [
    "Las transformaciones mas comunes que suceden en esta etapa usualmente son las que creamos anteriormente. En resumen,\n",
    "\n",
    "- Limpiar datos\n",
    "- Convertir variables numericas a la misma unidad\n",
    "- Unir datos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad6cc3e-c39a-4d47-aedb-a1893db562d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def order_and_merge(data_lists):\n",
    "    \n",
    "    pick_order = data_lists[0].columns\n",
    "    new_list = [d.reindex(columns=pick_order).sort_values(['date', 'hour']) for d in data_lists]\n",
    "    df = pd.concat(new_list)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418fd9be-307c-4b0b-9e35-34097139b1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "@task\n",
    "def transform(london, seoul, washington):\n",
    "    \n",
    "    london = (london.pipe(change_cols, london_cols)\n",
    "                    .pipe(add_dates_and_location, 'London', 'UK')\n",
    "                    .pipe(fix_and_drop, 'seasons', seasons_london, cols_drop_london))\n",
    "    \n",
    "    seoul = (seoul.pipe(change_cols, seoul_cols)\n",
    "                  .pipe(add_dates_and_location, 'Seoul', 'SK')\n",
    "                  .pipe(fix_and_drop, 'is_holiday', holidays_seoul, cols_drop_seoul))\n",
    "    \n",
    "    wash_dc = (washington.pipe(change_cols, wa_dc_cols)\n",
    "                         .pipe(add_dates_and_location, 'DC', 'USA')\n",
    "                         .pipe(fix_and_drop, 'seasons', seasons_wa_dc, cols_drop_wa_dc)\n",
    "                         .pipe(normalize_vars))\n",
    "    \n",
    "    return order_and_merge([london, seoul, wash_dc])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e1a7f2-056f-451b-9d58-7bf6542f3ea0",
   "metadata": {},
   "source": [
    "## 8. Descargar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785c3c96-c261-48d5-83ab-1fe30bf72307",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_table = SQLiteScript(\n",
    "    db=clean_db_path,\n",
    "    script=\"\"\"CREATE TABLE IF NOT EXISTS bike_sharing (date text, count integer, temperature real, humidity real,\n",
    "              wind_speed real, is_holiday real, is_weekend integer, seasons text, year integer,\n",
    "              month integer, week integer, day integer,hour integer, weekday integer, city text,\n",
    "              country text)\"\"\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3a717e-ad6a-4c03-9889-3d47be517c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "@task\n",
    "def load(data, path_and_name):\n",
    "    \n",
    "    data = list(data.itertuples(name='Bikes', index=False))\n",
    "    \n",
    "    insert_cmd = \"INSERT INTO bike_sharing VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\"\n",
    "    with closing(sqlite3.connect(path_and_name)) as conn:\n",
    "        with closing(conn.cursor()) as cursor:\n",
    "            cursor.executemany(insert_cmd, data)\n",
    "            conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ffe40b-8bd1-443c-8f95-5300f282e153",
   "metadata": {},
   "source": [
    "## 9. Lanza La Tuberia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4e9c43-4c96-47a7-9efe-47f90ce8383e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with Flow('bikes-ETL') as flow:\n",
    "    \n",
    "    the_table = new_table()\n",
    "    \n",
    "    london = extract_2(london_path)\n",
    "    seoul = extract_1(seoul_path)\n",
    "    wash_dc = extract_3(wash_dc_path)\n",
    "    \n",
    "    transformed = transform(london, seoul, wash_dc)\n",
    "        \n",
    "    data_loaded = load(transformed, clean_db_path)\n",
    "    data_loaded.set_upstream(the_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af9f4a4-c5a8-4c2b-b31c-d6da28fd8198",
   "metadata": {},
   "outputs": [],
   "source": [
    "flow.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83f8f69-f26b-41e8-a7aa-405be951e210",
   "metadata": {},
   "outputs": [],
   "source": [
    "flow.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60a30ab-e495-419c-a4f0-6bb6b38844ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_sql_query(\"SELECT * FROM bike_sharing\", sqlite3.connect(clean_db_path)).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714b1086-66ba-42db-b9dd-9929aefe1c82",
   "metadata": {},
   "source": [
    "## Ejercicio\n",
    "\n",
    "Cambia la funcion para descargar (`load()`) y haz que guarde los resultados en formato `parquet`. Corre la tuberia de nuevo y cerciorate de que los resultados sean iguales a los anteriores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8921afa-8dca-48e2-a8fb-2df5faef110a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ec06a2-5004-43cc-b657-03ec0f40394d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "265fafb5-db27-492a-b296-93a97c171624",
   "metadata": {},
   "source": [
    "## 10. Automatizar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a845e3d-3903-45ee-b44b-2eafbd328eda",
   "metadata": {},
   "source": [
    "Nuestra jefa nos comenta que los datos de las 3 ciudades seran actualizados todos los sabados asi que tenemos que automatizar el intervalo en el cual queremos que nuestro programa corra. Para ese tenoms la funcion de `IntervalSchedule` en prefect, y esta nos permite establecer el intervalo de tiempo que necesitamos. Ya sea un minuto, dos semanas, o un mes, anadir este detallo es algo trivial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f807b8c-7895-4f51-abfe-5c736ee05fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from prefect.schedules import IntervalSchedule\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865ae5fa-4b7f-4f53-91a2-5d787f182f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "schedule = IntervalSchedule(interval=datetime.timedelta(minutes=1), \n",
    "                            # start_date=datetime.datetime(2021, 11, 5)\n",
    "                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e100a922-4b46-4afa-8969-6f6a22c4b062",
   "metadata": {},
   "outputs": [],
   "source": [
    "with Flow('bikes-ETL', schedule=schedule) as flow:\n",
    "    \n",
    "    the_table = new_table()\n",
    "    \n",
    "    london = extract_2(london_path)\n",
    "    seoul = extract_1(seoul_path)\n",
    "    wash_dc = extract_3(wash_dc_path)\n",
    "    \n",
    "    transformed = transform(london, seoul, wash_dc)\n",
    "        \n",
    "    data_loaded = load(transformed, clean_db_path)\n",
    "    data_loaded.set_upstream(the_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9252d0e8-04fc-405f-b747-e9eebc29a0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "flow.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb0bb6e-9ca2-4a92-a859-ddf1630ec984",
   "metadata": {},
   "source": [
    "Para aprender mas sobre como programar y actualizar tus tuberias, por favor visita la documentacion oficial en [prefect schedules](https://docs.prefect.io/core/concepts/schedules.html#simple-schedules)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06074f70-0769-4ed1-8b56-6c15e43451ef",
   "metadata": {},
   "source": [
    "## 11. Resumen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa474f7-6585-42ee-a4df-3de9743fe279",
   "metadata": {},
   "source": [
    "1. Crear tuberias ETL te ayuda a ahorar tiempo con la limpieza de tus datos.\n",
    "2. pandas `pipe` te ayuda a crear cadenas de funciones y ahorrar tiempo y lineas de codigo.\n",
    "3. Prefect te ahora tiempo ya que encadena mas funciones por ti y te ayuda a crear horarios para tus funciones.\n",
    "4. No importa que tipo de profesional seas, mover y limpiar tus datos es una herramienta invaluable que no esta de mas saber."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
